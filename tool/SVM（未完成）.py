import numpy as np
import random


train_data = np.array([[-0.214824, 0.662756, -1.000000, ],
                       [-0.061569, -0.091875, 1.000000, ],
                       [0.406933, 0.648055, -1.000000, ],
                       [0.223650, 0.130142, 1.000000, ],
                       [0.231317, 0.766906, -1.000000, ],
                       [-0.748800, -0.531637, -1.000000, ],
                       [-0.557789, 0.375797, -1.000000, ],
                       [0.207123, -0.019463, 1.000000, ],
                       [0.286462, 0.719470, -1.000000, ],
                       [0.195300, -0.179039, 1.000000, ],
                       [-0.152696, -0.153030, 1.000000, ],
                       [0.384471, 0.653336, -1.000000, ],
                       [-0.117280, -0.153217, 1.000000, ],
                       [-0.238076, 0.000583, 1.000000, ],
                       [-0.413576, 0.145681, 1.000000, ],
                       [0.490767, -0.680029, -1.000000, ],
                       [0.199894, -0.199381, 1.000000, ],
                       [-0.356048, 0.537960, -1.000000, ],
                       [-0.392868, -0.125261, 1.000000, ],
                       [0.353588, -0.070617, 1.000000, ],
                       [0.020984, 0.925720, -1.000000, ],
                       [-0.475167, -0.346247, -1.000000, ],
                       [0.074952, 0.042783, 1.000000, ],
                       [0.394164, -0.058217, 1.000000, ],
                       [0.663418, 0.436525, -1.000000, ],
                       [0.402158, 0.577744, -1.000000, ],
                       [-0.449349, -0.038074, 1.000000, ],
                       [0.619080, -0.088188, -1.000000, ],
                       [0.268066, -0.071621, 1.000000, ],
                       [-0.015165, 0.359326, 1.000000, ],
                       [0.539368, -0.374972, -1.000000, ],
                       [-0.319153, 0.629673, -1.000000, ],
                       [0.694424, 0.641180, -1.000000, ],
                       [0.079522, 0.193198, 1.000000, ],
                       [0.253289, -0.285861, 1.000000, ],
                       [-0.035558, -0.010086, 1.000000, ],
                       [-0.403483, 0.474466, -1.000000, ],
                       [-0.034312, 0.995685, -1.000000, ],
                       [-0.590657, 0.438051, -1.000000, ],
                       [-0.098871, -0.023953, 1.000000, ],
                       [-0.250001, 0.141621, 1.000000, ],
                       [-0.012998, 0.525985, -1.000000, ],
                       [0.153738, 0.491531, -1.000000, ],
                       [0.388215, -0.656567, -1.000000, ],
                       [0.049008, 0.013499, 1.000000, ],
                       [0.068286, 0.392741, 1.000000, ],
                       [0.747800, -0.066630, -1.000000, ],
                       [0.004621, -0.042932, 1.000000, ],
                       [-0.701600, 0.190983, -1.000000, ],
                       [0.055413, -0.024380, 1.000000, ],
                       [0.035398, -0.333682, 1.000000, ],
                       [0.211795, 0.024689, 1.000000, ],
                       [-0.045677, 0.172907, 1.000000, ],
                       [0.595222, 0.209570, -1.000000, ],
                       [0.229465, 0.250409, 1.000000, ],
                       [-0.089293, 0.068198, 1.000000, ],
                       [0.384300, -0.176570, 1.000000, ],
                       [0.834912, -0.110321, -1.000000, ],
                       [-0.307768, 0.503038, -1.000000, ],
                       [-0.777063, -0.348066, -1.000000, ],
                       [0.017390, 0.152441, 1.000000, ],
                       [-0.293382, -0.139778, 1.000000, ],
                       [-0.203272, 0.286855, 1.000000, ],
                       [0.957812, -0.152444, -1.000000, ],
                       [0.004609, -0.070617, 1.000000, ],
                       [-0.755431, 0.096711, -1.000000, ],
                       [-0.526487, 0.547282, -1.000000, ],
                       [-0.246873, 0.833713, -1.000000, ],
                       [0.185639, -0.066162, 1.000000, ],
                       [0.851934, 0.456603, -1.000000, ],
                       [-0.827912, 0.117122, -1.000000, ],
                       [0.233512, -0.106274, 1.000000, ],
                       [0.583671, -0.709033, -1.000000, ],
                       [-0.487023, 0.625140, -1.000000, ],
                       [-0.448939, 0.176725, 1.000000, ],
                       [0.155907, -0.166371, 1.000000, ],
                       [0.334204, 0.381237, -1.000000, ],
                       [0.081536, -0.106212, 1.000000, ],
                       [0.227222, 0.527437, -1.000000, ],
                       [0.759290, 0.330720, -1.000000, ],
                       [0.204177, -0.023516, 1.000000, ],
                       [0.577939, 0.403784, -1.000000, ],
                       [-0.568534, 0.442948, -1.000000, ],
                       [-0.011520, 0.021165, 1.000000, ],
                       [0.875720, 0.422476, -1.000000, ],
                       [0.297885, -0.632874, -1.000000, ],
                       [-0.015821, 0.031226, 1.000000, ],
                       [0.541359, -0.205969, -1.000000, ],
                       [-0.689946, -0.508674, -1.000000, ],
                       [-0.343049, 0.841653, -1.000000, ],
                       [0.523902, -0.436156, -1.000000, ],
                       [0.249281, -0.711840, -1.000000, ],
                       [0.193449, 0.574598, -1.000000, ],
                       [-0.257542, -0.753885, -1.000000, ],
                       [-0.021605, 0.158080, 1.000000, ],
                       [0.601559, -0.727041, -1.000000, ],
                       [-0.791603, 0.095651, -1.000000, ],
                       [-0.908298, -0.053376, -1.000000, ],
                       [0.122020, 0.850966, -1.000000, ],
                       [-0.725568, -0.292022, -1.000000, ],
                       ])
test_data = np.array([[0.676771, -0.486687, -1.000000, ],
                      [0.008473, 0.186070, 1.000000, ],
                      [-0.727789, 0.594062, -1.000000, ],
                      [0.112367, 0.287852, 1.000000, ],
                      [0.383633, -0.038068, 1.000000, ],
                      [-0.927138, -0.032633, -1.000000, ],
                      [-0.842803, -0.423115, -1.000000, ],
                      [-0.003677, -0.367338, 1.000000, ],
                      [0.443211, -0.698469, -1.000000, ],
                      [-0.473835, 0.005233, 1.000000, ],
                      [0.616741, 0.590841, -1.000000, ],
                      [0.557463, -0.373461, -1.000000, ],
                      [-0.498535, -0.223231, -1.000000, ],
                      [-0.246744, 0.276413, 1.000000, ],
                      [-0.761980, -0.244188, -1.000000, ],
                      [0.641594, -0.479861, -1.000000, ],
                      [-0.659140, 0.529830, -1.000000, ],
                      [-0.054873, -0.238900, 1.000000, ],
                      [-0.089644, -0.244683, 1.000000, ],
                      [-0.431576, -0.481538, -1.000000, ],
                      [-0.099535, 0.728679, -1.000000, ],
                      [-0.188428, 0.156443, 1.000000, ],
                      [0.267051, 0.318101, 1.000000, ],
                      [0.222114, -0.528887, -1.000000, ],
                      [0.030369, 0.113317, 1.000000, ],
                      [0.392321, 0.026089, 1.000000, ],
                      [0.298871, -0.915427, -1.000000, ],
                      [-0.034581, -0.133887, 1.000000, ],
                      [0.405956, 0.206980, 1.000000, ],
                      [0.144902, -0.605762, -1.000000, ],
                      [0.274362, -0.401338, 1.000000, ],
                      [0.397998, -0.780144, -1.000000, ],
                      [0.037863, 0.155137, 1.000000, ],
                      [-0.010363, -0.004170, 1.000000, ],
                      [0.506519, 0.486619, -1.000000, ],
                      [0.000082, -0.020625, 1.000000, ],
                      [0.057761, -0.155140, 1.000000, ],
                      [0.027748, -0.553763, -1.000000, ],
                      [-0.413363, -0.746830, -1.000000, ],
                      [0.081500, -0.014264, 1.000000, ],
                      [0.047137, -0.491271, 1.000000, ],
                      [-0.267459, 0.024770, 1.000000, ],
                      [-0.148288, -0.532471, -1.000000, ],
                      [-0.225559, -0.201622, 1.000000, ],
                      [0.772360, -0.518986, -1.000000, ],
                      [-0.440670, 0.688739, -1.000000, ],
                      [0.329064, -0.095349, 1.000000, ],
                      [0.970170, -0.010671, -1.000000, ],
                      [-0.689447, -0.318722, -1.000000, ],
                      [-0.465493, -0.227468, -1.000000, ],
                      [-0.049370, 0.405711, 1.000000, ],
                      [-0.166117, 0.274807, 1.000000, ],
                      [0.054483, 0.012643, 1.000000, ],
                      [0.021389, 0.076125, 1.000000, ],
                      [-0.104404, -0.914042, -1.000000, ],
                      [0.294487, 0.440886, -1.000000, ],
                      [0.107915, -0.493703, -1.000000, ],
                      [0.076311, 0.438860, 1.000000, ],
                      [0.370593, -0.728737, -1.000000, ],
                      [0.409890, 0.306851, -1.000000, ],
                      [0.285445, 0.474399, -1.000000, ],
                      [-0.870134, -0.161685, -1.000000, ],
                      [-0.654144, -0.675129, -1.000000, ],
                      [0.285278, -0.767310, -1.000000, ],
                      [0.049548, -0.000907, 1.000000, ],
                      [0.030014, -0.093265, 1.000000, ],
                      [-0.128859, 0.278865, 1.000000, ],
                      [0.307463, 0.085667, 1.000000, ],
                      [0.023440, 0.298638, 1.000000, ],
                      [0.053920, 0.235344, 1.000000, ],
                      [0.059675, 0.533339, -1.000000, ],
                      [0.817125, 0.016536, -1.000000, ],
                      [-0.108771, 0.477254, 1.000000, ],
                      [-0.118106, 0.017284, 1.000000, ],
                      [0.288339, 0.195457, 1.000000, ],
                      [0.567309, -0.200203, -1.000000, ],
                      [-0.202446, 0.409387, 1.000000, ],
                      [-0.330769, -0.240797, 1.000000, ],
                      [-0.422377, 0.480683, -1.000000, ],
                      [-0.295269, 0.326017, 1.000000, ],
                      [0.261132, 0.046478, 1.000000, ],
                      [-0.492244, -0.319998, -1.000000, ],
                      [-0.384419, 0.099170, 1.000000, ],
                      [0.101882, -0.781145, -1.000000, ],
                      [0.234592, -0.383446, 1.000000, ],
                      [-0.020478, -0.901833, -1.000000, ],
                      [0.328449, 0.186633, 1.000000, ],
                      [-0.150059, -0.409158, 1.000000, ],
                      [-0.155876, -0.843413, -1.000000, ],
                      [-0.098134, -0.136786, 1.000000, ],
                      [0.110575, -0.197205, 1.000000, ],
                      [0.219021, 0.054347, 1.000000, ],
                      [0.030152, 0.251682, 1.000000, ],
                      [0.033447, -0.122824, 1.000000, ],
                      [-0.686225, -0.020779, -1.000000, ],
                      [-0.911211, -0.262011, -1.000000, ],
                      [0.572557, 0.377526, -1.000000, ],
                      [-0.073647, -0.519163, -1.000000, ],
                      [-0.281830, -0.797236, -1.000000, ],
                      [-0.555263, 0.126232, -1.000000, ], ])


class SVM():
    def __init__(self):
        pass

    def randSelect(self, i, m):
        #  在0-m中随机选择一个不是i的整数
        j = int(random.uniform(0,m-1))
        return j if j < i else 1+j

    def smoP(self,data,labels,C,threshold,maxIter,kTup=('linear',1)):
        '''
        # SMO函数，用于快速求解出alpha
        :param data: np.array 数据特征,
        :param labels: 数据类别
        :param C:  参数C
        :param toler: 阀值toler，
        :param maxIter: 最大迭代次数
        :param kTup: 核函数（默认线性核）
        :return:
        '''
        self.optStruct(data,labels,C,threshold,kTup)
        iter = 0
        entireSet = True
        while (iter < maxIter) and ((alphaPairsChanged > 0) or (entireSet)):
            alphaPairsChanged = 0
            if entireSet:
                # 遍历所有数据
                alphaPairsChanged += self.innerL()


    def optStruct(self, data, labels, C, threshold, kTup):
        self.data = data
        self.labels = labels
        self.C = C  #软间隔参数C，参数越大，非线性拟合能力越强
        self.threshold = threshold #停止阀值
        self.nums = data.shape[0]
        self.alpha = np.zeros((self.nums))
        self.b = 0
        self.cache = np.zeros((self.nums ,2))
        self.K = self.KernelTrans(kTup) #核函数的计算结果

    def KernelTrans(self, kTup):
        # 核函数，输入参数,X:支持向量的特征树；A：某一行特征数据；kTup：('linear',k1)核函数的类型和参数
        K = np.zeros((self.nums,1))
        if kTup[0] == 'linear':
            K = np.dot(self.data,self.data.T) * kTup[1]
        elif kTup[0] == 'rbf':
            sigma = kTup[1]
            z = self.data - self.data.mean(axis=1)
            K = np.exp((np.dot(z,z.T)/sigma))
        elif kTup[0] == 'ploy':
            _ , c ,d = kTup
            K =(np.dot(self.data, self.data.T) + c)**d
        else:
            raise NameError("Not realized %s!"%kTup[0])
        return K

    def innerL(self):
        # 首先检验ai是否满足KKT条件，如果不满足，随机选择aj进行优化，更新ai,aj,b值
        E = self.clacEK()
        if (np.dot(self.labels,E ) < -self.threshold and min(self.alpha) < self.C) or (np.dot(self.labels , E)>self.threshold and all(self.alpha >0)): #检验这行数据是否符合KKT条件 参考《统计学习方法》p128公式7.111-113
            pass

    def clacEK(self):
        # （参考《统计学习方法》p127公式7.105）
        fx = np.dot(self.alpha * self.labels,self.K) +self.b
        Ek = fx -self.labels
        return Ek


import matplotlib.pyplot as plt
class Hard_Margin_SVM:
    def __init__(self, visualization=True):
        self.visualization = visualization
        self.colors = {1: 'r', -1: 'g'}
        if self.visualization:
            self.fig = plt.figure()
            self.ax = self.fig.add_subplot(1, 1, 1)

    # 定义训练函数
    def train(self, data):
        self.data = data
        # 参数字典 { ||w||: [w,b] }
        opt_dict = {}

        # 数据转换列表
        transforms = [[1, 1],
                      [-1, 1],
                      [-1, -1],
                      [1, -1]]

        # 从字典中获取所有数据
        all_data = []
        for yi in self.data:
            for featureset in self.data[yi]:
                for feature in featureset:
                    all_data.append(feature)

        # 获取数据最大最小值
        self.max_feature_value = max(all_data)
        self.min_feature_value = min(all_data)
        all_data = None

        # 定义一个学习率(步长)列表
        step_sizes = [self.max_feature_value * 0.1,
                      self.max_feature_value * 0.01,
                      self.max_feature_value * 0.001
                      ]

        # 参数b的范围设置
        b_range_multiple = 2
        b_multiple = 5
        latest_optimum = self.max_feature_value * 10

        # 基于不同步长训练优化
        for step in step_sizes:
            w = np.array([latest_optimum, latest_optimum])
            # 凸优化
            optimized = False
            while not optimized:
                for b in np.arange(-1 * (self.max_feature_value * b_range_multiple),
                                   self.max_feature_value * b_range_multiple,
                                   step * b_multiple):
                    for transformation in transforms:
                        w_t = w * transformation
                        found_option = True

                        for i in self.data:
                            for xi in self.data[i]:
                                yi = i
                                if not yi * (np.dot(w_t, xi) + b) >= 1:
                                    found_option = False
                                    # print(xi,':',yi*(np.dot(w_t,xi)+b))

                        if found_option:
                            opt_dict[np.linalg.norm(w_t)] = [w_t, b]

                if w[0] < 0:
                    optimized = True
                    print('Optimized a step!')
                else:
                    w = w - step

            norms = sorted([n for n in opt_dict])
            # ||w|| : [w,b]
            opt_choice = opt_dict[norms[0]]
            self.w = opt_choice[0]
            self.b = opt_choice[1]
            latest_optimum = opt_choice[0][0] + step * 2

        for i in self.data:
            for xi in self.data[i]:
                yi = i
                print(xi, ':', yi * (np.dot(self.w, xi) + self.b))

                # 定义预测函数

    def predict(self, features):
        # sign( x.w+b )
        classification = np.sign(np.dot(np.array(features), self.w) + self.b)
        if classification != 0 and self.visualization:
            self.ax.scatter(features[0], features[1], s=200, marker='^', c=self.colors[classification])
        return classification

    # 定义结果绘图函数
    def visualize(self):
        [[self.ax.scatter(x[0], x[1], s=100, color=self.colors[i]) for x in data_dict[i]] for i in data_dict]

        # hyperplane = x.w+b
        # v = x.w+b
        # psv = 1
        # nsv = -1
        # dec = 0
        # 定义线性超平面
        def hyperplane(x, w, b, v):
            return (-w[0] * x - b + v) / w[1]

        datarange = (self.min_feature_value * 0.9, self.max_feature_value * 1.1)
        hyp_x_min = datarange[0]
        hyp_x_max = datarange[1]

        # (w.x+b) = 1
        # 正支持向量
        psv1 = hyperplane(hyp_x_min, self.w, self.b, 1)
        psv2 = hyperplane(hyp_x_max, self.w, self.b, 1)
        self.ax.plot([hyp_x_min, hyp_x_max], [psv1, psv2], 'k')

        # (w.x+b) = -1
        # 负支持向量
        nsv1 = hyperplane(hyp_x_min, self.w, self.b, -1)
        nsv2 = hyperplane(hyp_x_max, self.w, self.b, -1)
        self.ax.plot([hyp_x_min, hyp_x_max], [nsv1, nsv2], 'k')

        # (w.x+b) = 0
        # 线性分隔超平面
        db1 = hyperplane(hyp_x_min, self.w, self.b, 0)
        db2 = hyperplane(hyp_x_max, self.w, self.b, 0)
        self.ax.plot([hyp_x_min, hyp_x_max], [db1, db2], 'y--')

        plt.show()
if __name__ == '__main__':
    print(test_data[:,2].shape)

    data_dict = {-1: np.array([[1, 7],
                               [2, 8],
                               [3, 8], ]),

                 1: np.array([[5, 1],
                              [6, -1],
                              [7, 3], ])}

    svm = Hard_Margin_SVM()
    svm.train(data=data_dict)

    predict_us = [[0, 10],
                  [1, 3],
                  [3, 4],
                  [3, 5],
                  [5, 5],
                  [5, 6],
                  [6, -5],
                  [5, 8],
                  [2, 5],
                  [8, -3]]

    for p in predict_us:
        svm.predict(p)

    svm.visualize()